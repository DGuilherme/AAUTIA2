{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "run_doc2vec_lee.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DGuilherme/AAUTIA2/blob/main/run_doc2vec_lee.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65qUgcEEP3Or"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khegv08zP3Ow"
      },
      "source": [
        "\n",
        "Doc2Vec Model\n",
        "=============\n",
        "\n",
        "Introduces Gensim's Doc2Vec model and demonstrates its use on the\n",
        "`Lee Corpus <https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`__.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDogFk98L7tP"
      },
      "source": [
        "# Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qO9DUR65P3Ox"
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBpJZNWhL36v"
      },
      "source": [
        "# CH3 imports\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08lzzPRKP3Ox"
      },
      "source": [
        "Doc2Vec is a `core_concepts_model` that represents each\n",
        "`core_concepts_document` as a `core_concepts_vector`.  This\n",
        "tutorial introduces the model and demonstrates how to train and assess it.\n",
        "\n",
        "Here's a list of what we'll be doing:\n",
        "\n",
        "0. Review the relevant models: bag-of-words, Word2Vec, Doc2Vec\n",
        "1. Load and preprocess the training and test corpora (see `core_concepts_corpus`)\n",
        "2. Train a Doc2Vec `core_concepts_model` model using the training corpus\n",
        "3. Demonstrate how the trained model can be used to infer a `core_concepts_vector`\n",
        "4. Assess the model\n",
        "5. Test the model on the test corpus\n",
        "\n",
        "Review: Bag-of-words\n",
        "--------------------\n",
        "\n",
        ".. Note:: Feel free to skip these review sections if you're already familiar with the models.\n",
        "\n",
        "You may be familiar with the `bag-of-words model\n",
        "<https://en.wikipedia.org/wiki/Bag-of-words_model>`_ from the\n",
        "`core_concepts_vector` section.\n",
        "This model transforms each document to a fixed-length vector of integers.\n",
        "For example, given the sentences:\n",
        "\n",
        "- ``John likes to watch movies. Mary likes movies too.``\n",
        "- ``John also likes to watch football games. Mary hates football.``\n",
        "\n",
        "The model outputs the vectors:\n",
        "\n",
        "- ``[1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0]``\n",
        "- ``[1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1]``\n",
        "\n",
        "Each vector has 10 elements, where each element counts the number of times a\n",
        "particular word occurred in the document.\n",
        "The order of elements is arbitrary.\n",
        "In the example above, the order of the elements corresponds to the words:\n",
        "``[\"John\", \"likes\", \"to\", \"watch\", \"movies\", \"Mary\", \"too\", \"also\", \"football\", \"games\", \"hates\"]``.\n",
        "\n",
        "Bag-of-words models are surprisingly effective, but have several weaknesses.\n",
        "\n",
        "First, they lose all information about word order: \"John likes Mary\" and\n",
        "\"Mary likes John\" correspond to identical vectors. There is a solution: bag\n",
        "of `n-grams <https://en.wikipedia.org/wiki/N-gram>`__\n",
        "models consider word phrases of length n to represent documents as\n",
        "fixed-length vectors to capture local word order but suffer from data\n",
        "sparsity and high dimensionality.\n",
        "\n",
        "Second, the model does not attempt to learn the meaning of the underlying\n",
        "words, and as a consequence, the distance between vectors doesn't always\n",
        "reflect the difference in meaning.  The ``Word2Vec`` model addresses this\n",
        "second problem.\n",
        "\n",
        "Review: ``Word2Vec`` Model\n",
        "--------------------------\n",
        "\n",
        "``Word2Vec`` is a more recent model that embeds words in a lower-dimensional\n",
        "vector space using a shallow neural network. The result is a set of\n",
        "word-vectors where vectors close together in vector space have similar\n",
        "meanings based on context, and word-vectors distant to each other have\n",
        "differing meanings. For example, ``strong`` and ``powerful`` would be close\n",
        "together and ``strong`` and ``Paris`` would be relatively far.\n",
        "\n",
        "Gensim's :py:class:`~gensim.models.word2vec.Word2Vec` class implements this model.\n",
        "\n",
        "With the ``Word2Vec`` model, we can calculate the vectors for each **word** in a document.\n",
        "But what if we want to calculate a vector for the **entire document**\\ ?\n",
        "We could average the vectors for each word in the document - while this is quick and crude, it can often be useful.\n",
        "However, there is a better way...\n",
        "\n",
        "Introducing: Paragraph Vector\n",
        "-----------------------------\n",
        "\n",
        ".. Important:: In Gensim, we refer to the Paragraph Vector model as ``Doc2Vec``.\n",
        "\n",
        "Le and Mikolov in 2014 introduced the `Doc2Vec algorithm <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`__,\n",
        "which usually outperforms such simple-averaging of ``Word2Vec`` vectors.\n",
        "\n",
        "The basic idea is: act as if a document has another floating word-like\n",
        "vector, which contributes to all training predictions, and is updated like\n",
        "other word-vectors, but we will call it a doc-vector. Gensim's\n",
        ":py:class:`~gensim.models.doc2vec.Doc2Vec` class implements this algorithm.\n",
        "\n",
        "There are two implementations:\n",
        "\n",
        "1. Paragraph Vector - Distributed Memory (PV-DM)\n",
        "2. Paragraph Vector - Distributed Bag of Words (PV-DBOW)\n",
        "\n",
        ".. Important::\n",
        "  Don't let the implementation details below scare you.\n",
        "  They're advanced material: if it's too much, then move on to the next section.\n",
        "\n",
        "PV-DM is analogous to Word2Vec CBOW. The doc-vectors are obtained by training\n",
        "a neural network on the synthetic task of predicting a center word based an\n",
        "average of both context word-vectors and the full document's doc-vector.\n",
        "\n",
        "PV-DBOW is analogous to Word2Vec SG. The doc-vectors are obtained by training\n",
        "a neural network on the synthetic task of predicting a target word just from\n",
        "the full document's doc-vector. (It is also common to combine this with\n",
        "skip-gram testing, using both the doc-vector and nearby word-vectors to\n",
        "predict a single target word, but only one at a time.)\n",
        "\n",
        "Prepare the Training and Test Data\n",
        "----------------------------------\n",
        "\n",
        "For this tutorial, we'll be training our model using the `Lee Background\n",
        "Corpus\n",
        "<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_\n",
        "included in gensim. This corpus contains 314 documents selected from the\n",
        "Australian Broadcasting Corporationâ€™s news mail service, which provides text\n",
        "e-mails of headline stories and covers a number of broad topics.\n",
        "\n",
        "And we'll test our model by eye using the much shorter `Lee Corpus\n",
        "<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_\n",
        "which contains 50 documents.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-HmJnVDt3XM",
        "outputId": "02af797c-cd62-4299-b363-d1ee924409cc"
      },
      "source": [
        "# resolver problema de gensim version\n",
        "!pip install --upgrade gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/52/f1417772965652d4ca6f901515debcd9d6c5430969e8c02ee7737e6de61c/gensim-4.0.1-cp37-cp37m-manylinux1_x86_64.whl (23.9MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23.9MB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (4.2.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHWiNSEHLY9Y",
        "outputId": "9945aa26-40c3-4c8b-b90e-323d65437b8a"
      },
      "source": [
        "# Dataset Chalenge 3\n",
        "# raw: https://raw.githubusercontent.com/DGuilherme/AAUTIA2/main/Doc2Vec/dataset/dataset.txt\n",
        "\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/DGuilherme/AAUTIA2/main/Doc2Vec/dataset/dataset.txt'\n",
        "train_data = pd.read_csv(url,delimiter=r\"|\")\n",
        "# Dataset is now stored in a Pandas Dataframe\n",
        "print(train_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                            ID  ...                                             Resumo\n",
            "0    57025eb6eb1ec9f5515f7c33   ...   A damper for use in submerged hydrophone susp...\n",
            "1    57025eb6eb1ec9f5515f7c34   ...   A flexible longitudinally continuous tape con...\n",
            "2    57025eb6eb1ec9f5515f7c35   ...   The method of packaging perishable products i...\n",
            "3    57025eb6eb1ec9f5515f7c36   ...   An improvement in a tampon for absorbing mens...\n",
            "4    57025eb6eb1ec9f5515f7c37   ...   A heavy duty motor vehicle has a main frame w...\n",
            "..                         ...  ...                                                ...\n",
            "495  57025eb6eb1ec9f5515f7e22   ...   A peristaltic pump comprising a portable hous...\n",
            "496  57025eb6eb1ec9f5515f7e23   ...   A gas compressor or blower comprising a cylin...\n",
            "497  57025eb6eb1ec9f5515f7e24   ...   A rotary cell pump is provided for the convey...\n",
            "498  57025eb6eb1ec9f5515f7e25   ...   An overspeed shutoff device for use in a rota...\n",
            "499  57025eb6eb1ec9f5515f7e26   ...   A rotary positive displacement pump of the in...\n",
            "\n",
            "[500 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "VPdamxHEMO39",
        "outputId": "e6c7d0b9-09c9-46bb-a7ee-37534843185f"
      },
      "source": [
        "train_data.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Titulo</th>\n",
              "      <th>Resumo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>500</td>\n",
              "      <td>500</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>500</td>\n",
              "      <td>497</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>57025eb6eb1ec9f5515f7cd9</td>\n",
              "      <td>Heat exchanger</td>\n",
              "      <td>A small portable motorized attachment for pro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               ID  ...                                             Resumo\n",
              "count                         500  ...                                                500\n",
              "unique                        500  ...                                                500\n",
              "top     57025eb6eb1ec9f5515f7cd9   ...   A small portable motorized attachment for pro...\n",
              "freq                            1  ...                                                  1\n",
              "\n",
              "[4 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb4S7aK_gn1P"
      },
      "source": [
        "\n",
        "import gensim\n",
        "def tagData(dataframe):\n",
        "  for index,row in dataframe.iterrows():\n",
        "    tokens = gensim.utils.simple_preprocess(row['Resumo'])\n",
        "    yield gensim.models.doc2vec.TaggedDocument(tokens, [index]) #yield gensim.utils.simple_preprocess(row['Resumo'])\n",
        "\n",
        "\n",
        "vocabulary = list(tagData(train_data))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhtZj7dsP3O1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e306a6e4-75ad-4a06-8623-cab2c4526f7f"
      },
      "source": [
        "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40) # Create inital empty model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-08 22:23:34,817 : INFO : Doc2Vec lifecycle event {'params': 'Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3)', 'datetime': '2021-04-08T22:23:34.817860', 'gensim': '4.0.1', 'python': '3.7.10 (default, Feb 20 2021, 21:17:23) \\n[GCC 7.5.0]', 'platform': 'Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic', 'event': 'created'}\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BRua8p_P3O1"
      },
      "source": [
        "Build a vocabulary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEto6_mIP3O1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9843b956-6943-4afb-ba0a-089926a7538e"
      },
      "source": [
        "model.build_vocab(vocabulary) # Add data to the model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-08 22:25:07,519 : INFO : collecting all words and their counts\n",
            "2021-04-08 22:25:07,521 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
            "2021-04-08 22:25:07,542 : INFO : collected 5094 word types and 500 unique tags from a corpus of 500 examples and 56523 words\n",
            "2021-04-08 22:25:07,543 : INFO : Creating a fresh vocabulary\n",
            "2021-04-08 22:25:07,561 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 3187 unique words (62.563800549666276%% of original 5094, drops 1907)', 'datetime': '2021-04-08T22:25:07.561467', 'gensim': '4.0.1', 'python': '3.7.10 (default, Feb 20 2021, 21:17:23) \\n[GCC 7.5.0]', 'platform': 'Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic', 'event': 'prepare_vocab'}\n",
            "2021-04-08 22:25:07,563 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 54616 word corpus (96.62615218583585%% of original 56523, drops 1907)', 'datetime': '2021-04-08T22:25:07.563091', 'gensim': '4.0.1', 'python': '3.7.10 (default, Feb 20 2021, 21:17:23) \\n[GCC 7.5.0]', 'platform': 'Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic', 'event': 'prepare_vocab'}\n",
            "2021-04-08 22:25:07,594 : INFO : deleting the raw counts dictionary of 5094 items\n",
            "2021-04-08 22:25:07,596 : INFO : sample=0.001 downsamples 38 most-common words\n",
            "2021-04-08 22:25:07,597 : INFO : Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 39500.12174778367 word corpus (72.3%% of prior 54616)', 'datetime': '2021-04-08T22:25:07.597786', 'gensim': '4.0.1', 'python': '3.7.10 (default, Feb 20 2021, 21:17:23) \\n[GCC 7.5.0]', 'platform': 'Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic', 'event': 'prepare_vocab'}\n",
            "2021-04-08 22:25:07,643 : INFO : estimated required memory for 3187 words and 50 dimensions: 3068300 bytes\n",
            "2021-04-08 22:25:07,645 : INFO : resetting layer weights\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc0tvlEpP3O2"
      },
      "source": [
        "Essentially, the vocabulary is a list (accessible via\n",
        "``model.wv.index_to_key``) of all of the unique words extracted from the training corpus.\n",
        "Additional attributes for each word are available using the ``model.wv.get_vecattr()`` method,\n",
        "For example, to see how many times ``penalty`` appeared in the training corpus:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdLZLOSgP3O2",
        "outputId": "39e8b5b9-cb8e-40d0-83d1-69d96366ade4"
      },
      "source": [
        "print(f\"Word 'penalty' appeared {model.wv.get_vecattr('penalty', 'count')} times in the training corpus.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word 'penalty' appeared 4 times in the training corpus.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxqeEwfjP3O2"
      },
      "source": [
        "Next, train the model on the corpus.\n",
        "If optimized Gensim (with BLAS library) is being used, this should take no more than 3 seconds.\n",
        "If the BLAS library is not being used, this should take no more than 2\n",
        "minutes, so use optimized Gensim with BLAS if you value your time.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grRwV9b8P3O2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbd5af1d-d811-4170-c54f-2f8810f0125c"
      },
      "source": [
        "model.train(vocabulary, total_examples=model.corpus_count, epochs=model.epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-08 22:25:16,460 : INFO : Doc2Vec lifecycle event {'msg': 'training model with 3 workers on 3187 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5', 'datetime': '2021-04-08T22:25:16.460197', 'gensim': '4.0.1', 'python': '3.7.10 (default, Feb 20 2021, 21:17:23) \\n[GCC 7.5.0]', 'platform': 'Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic', 'event': 'train'}\n",
            "2021-04-08 22:25:16,603 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:16,612 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:16,614 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:16,618 : INFO : EPOCH - 1 : training on 56523 raw words (40040 effective words) took 0.1s, 275971 effective words/s\n",
            "2021-04-08 22:25:16,724 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:16,735 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:16,748 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:16,749 : INFO : EPOCH - 2 : training on 56523 raw words (40059 effective words) took 0.1s, 324790 effective words/s\n",
            "2021-04-08 22:25:16,854 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:16,872 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:16,877 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:16,879 : INFO : EPOCH - 3 : training on 56523 raw words (40072 effective words) took 0.1s, 334401 effective words/s\n",
            "2021-04-08 22:25:17,003 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:17,014 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:17,020 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:17,022 : INFO : EPOCH - 4 : training on 56523 raw words (39986 effective words) took 0.1s, 296914 effective words/s\n",
            "2021-04-08 22:25:17,136 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:17,145 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:17,154 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:17,156 : INFO : EPOCH - 5 : training on 56523 raw words (40067 effective words) took 0.1s, 315276 effective words/s\n",
            "2021-04-08 22:25:17,264 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:17,274 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:17,283 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:17,285 : INFO : EPOCH - 6 : training on 56523 raw words (40018 effective words) took 0.1s, 325677 effective words/s\n",
            "2021-04-08 22:25:17,394 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:17,403 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:17,409 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:17,411 : INFO : EPOCH - 7 : training on 56523 raw words (40024 effective words) took 0.1s, 337441 effective words/s\n",
            "2021-04-08 22:25:17,526 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:17,536 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:17,542 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:17,544 : INFO : EPOCH - 8 : training on 56523 raw words (39945 effective words) took 0.1s, 316267 effective words/s\n",
            "2021-04-08 22:25:17,655 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:17,670 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:17,678 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:17,679 : INFO : EPOCH - 9 : training on 56523 raw words (40009 effective words) took 0.1s, 314966 effective words/s\n",
            "2021-04-08 22:25:17,786 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:17,799 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:17,804 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:17,806 : INFO : EPOCH - 10 : training on 56523 raw words (39984 effective words) took 0.1s, 337985 effective words/s\n",
            "2021-04-08 22:25:17,935 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:17,944 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:17,949 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:17,950 : INFO : EPOCH - 11 : training on 56523 raw words (39962 effective words) took 0.1s, 309230 effective words/s\n",
            "2021-04-08 22:25:18,071 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:18,079 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:18,088 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:18,089 : INFO : EPOCH - 12 : training on 56523 raw words (40132 effective words) took 0.1s, 303188 effective words/s\n",
            "2021-04-08 22:25:18,205 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:18,218 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:18,224 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:18,226 : INFO : EPOCH - 13 : training on 56523 raw words (39991 effective words) took 0.1s, 313056 effective words/s\n",
            "2021-04-08 22:25:18,347 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:18,354 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:18,361 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:18,363 : INFO : EPOCH - 14 : training on 56523 raw words (39982 effective words) took 0.1s, 310617 effective words/s\n",
            "2021-04-08 22:25:18,484 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:18,493 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:18,503 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:18,504 : INFO : EPOCH - 15 : training on 56523 raw words (40058 effective words) took 0.1s, 293921 effective words/s\n",
            "2021-04-08 22:25:18,620 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:18,630 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:18,638 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:18,639 : INFO : EPOCH - 16 : training on 56523 raw words (40049 effective words) took 0.1s, 308779 effective words/s\n",
            "2021-04-08 22:25:18,757 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:18,766 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:18,775 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:18,777 : INFO : EPOCH - 17 : training on 56523 raw words (40083 effective words) took 0.1s, 309438 effective words/s\n",
            "2021-04-08 22:25:18,896 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:18,907 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:18,909 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:18,912 : INFO : EPOCH - 18 : training on 56523 raw words (39998 effective words) took 0.1s, 311648 effective words/s\n",
            "2021-04-08 22:25:19,031 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:19,038 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:19,045 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:19,049 : INFO : EPOCH - 19 : training on 56523 raw words (39950 effective words) took 0.1s, 312435 effective words/s\n",
            "2021-04-08 22:25:19,163 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:19,167 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:19,178 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:19,179 : INFO : EPOCH - 20 : training on 56523 raw words (40034 effective words) took 0.1s, 332165 effective words/s\n",
            "2021-04-08 22:25:19,292 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:19,301 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:19,307 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:19,308 : INFO : EPOCH - 21 : training on 56523 raw words (39999 effective words) took 0.1s, 322517 effective words/s\n",
            "2021-04-08 22:25:19,427 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:19,434 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:19,445 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:19,446 : INFO : EPOCH - 22 : training on 56523 raw words (40013 effective words) took 0.1s, 312250 effective words/s\n",
            "2021-04-08 22:25:19,548 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:19,574 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:19,585 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:19,587 : INFO : EPOCH - 23 : training on 56523 raw words (39883 effective words) took 0.1s, 295946 effective words/s\n",
            "2021-04-08 22:25:19,699 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:19,714 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:19,725 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:19,726 : INFO : EPOCH - 24 : training on 56523 raw words (40001 effective words) took 0.1s, 303695 effective words/s\n",
            "2021-04-08 22:25:19,836 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:19,845 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:19,851 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:19,854 : INFO : EPOCH - 25 : training on 56523 raw words (40055 effective words) took 0.1s, 328879 effective words/s\n",
            "2021-04-08 22:25:19,981 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:19,984 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:19,991 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:19,994 : INFO : EPOCH - 26 : training on 56523 raw words (39891 effective words) took 0.1s, 301385 effective words/s\n",
            "2021-04-08 22:25:20,108 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:20,116 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:20,122 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:20,123 : INFO : EPOCH - 27 : training on 56523 raw words (40048 effective words) took 0.1s, 321605 effective words/s\n",
            "2021-04-08 22:25:20,229 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:20,241 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:20,250 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:20,252 : INFO : EPOCH - 28 : training on 56523 raw words (39922 effective words) took 0.1s, 323910 effective words/s\n",
            "2021-04-08 22:25:20,364 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:20,370 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:20,384 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:20,385 : INFO : EPOCH - 29 : training on 56523 raw words (40169 effective words) took 0.1s, 320248 effective words/s\n",
            "2021-04-08 22:25:20,494 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:20,509 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:20,511 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:20,516 : INFO : EPOCH - 30 : training on 56523 raw words (39997 effective words) took 0.1s, 324016 effective words/s\n",
            "2021-04-08 22:25:20,645 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:20,648 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:20,650 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:20,651 : INFO : EPOCH - 31 : training on 56523 raw words (40005 effective words) took 0.1s, 310105 effective words/s\n",
            "2021-04-08 22:25:20,773 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:20,790 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:20,793 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:20,797 : INFO : EPOCH - 32 : training on 56523 raw words (39906 effective words) took 0.1s, 294406 effective words/s\n",
            "2021-04-08 22:25:20,922 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:20,931 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:20,940 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:20,941 : INFO : EPOCH - 33 : training on 56523 raw words (40066 effective words) took 0.1s, 293382 effective words/s\n",
            "2021-04-08 22:25:21,053 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:21,064 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:21,071 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:21,072 : INFO : EPOCH - 34 : training on 56523 raw words (39893 effective words) took 0.1s, 321300 effective words/s\n",
            "2021-04-08 22:25:21,193 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:21,196 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:21,200 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:21,201 : INFO : EPOCH - 35 : training on 56523 raw words (40059 effective words) took 0.1s, 328486 effective words/s\n",
            "2021-04-08 22:25:21,329 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:21,332 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:21,344 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:21,346 : INFO : EPOCH - 36 : training on 56523 raw words (39950 effective words) took 0.1s, 289795 effective words/s\n",
            "2021-04-08 22:25:21,464 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:21,471 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:21,473 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:21,476 : INFO : EPOCH - 37 : training on 56523 raw words (39969 effective words) took 0.1s, 341931 effective words/s\n",
            "2021-04-08 22:25:21,589 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:21,602 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:21,614 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:21,616 : INFO : EPOCH - 38 : training on 56523 raw words (40028 effective words) took 0.1s, 307586 effective words/s\n",
            "2021-04-08 22:25:21,724 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:21,740 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:21,750 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:21,751 : INFO : EPOCH - 39 : training on 56523 raw words (39960 effective words) took 0.1s, 309767 effective words/s\n",
            "2021-04-08 22:25:21,865 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 22:25:21,875 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 22:25:21,880 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 22:25:21,881 : INFO : EPOCH - 40 : training on 56523 raw words (39927 effective words) took 0.1s, 327465 effective words/s\n",
            "2021-04-08 22:25:21,886 : INFO : Doc2Vec lifecycle event {'msg': 'training on 2260920 raw words (1600184 effective words) took 5.4s, 295007 effective words/s', 'datetime': '2021-04-08T22:25:21.886452', 'gensim': '4.0.1', 'python': '3.7.10 (default, Feb 20 2021, 21:17:23) \\n[GCC 7.5.0]', 'platform': 'Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic', 'event': 'train'}\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmmWnuk9P3O2"
      },
      "source": [
        "Now, we can use the trained model to infer a vector for any piece of text\n",
        "by passing a list of words to the ``model.infer_vector`` function. This\n",
        "vector can then be compared with other vectors via cosine similarity.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnIJwDw4P3O2",
        "outputId": "cb8339af-9589-4a86-a96a-77d5bae44807"
      },
      "source": [
        "vector = model.infer_vector(['overspeed', 'shutoff' ,'device', 'for' ,'use'])\n",
        "print(vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.0380471   0.07107193 -0.16104157 -0.05671706 -0.11206745 -0.25842693\n",
            "  0.09781607  0.21397597 -0.3734844   0.05311184 -0.0563758   0.00350833\n",
            "  0.08645441  0.10875712 -0.2533288   0.09351682  0.24101683  0.13937809\n",
            " -0.23678514 -0.05698922 -0.12249822  0.0262745   0.20443738 -0.06480797\n",
            "  0.25651255  0.06226546  0.09046153 -0.11489527  0.00764522 -0.05704122\n",
            "  0.12138305  0.3093298  -0.04058864  0.02917185 -0.00899924  0.20694292\n",
            "  0.15399466 -0.222546    0.0661717   0.03404893  0.21823987  0.02295602\n",
            "  0.09952864 -0.18332772  0.30713084  0.04149257 -0.13677527 -0.25025985\n",
            "  0.2873318   0.15527335]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_ioe4QEP3O3"
      },
      "source": [
        "Note that ``infer_vector()`` does *not* take a string, but rather a list of\n",
        "string tokens, which should have already been tokenized the same way as the\n",
        "``words`` property of original training document objects.\n",
        "\n",
        "Also note that because the underlying training/inference algorithms are an\n",
        "iterative approximation problem that makes use of internal randomization,\n",
        "repeated inferences of the same text will return slightly different vectors.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcsmGSlLP3O3"
      },
      "source": [
        "Assessing the Model\n",
        "-------------------\n",
        "\n",
        "To assess our new model, we'll first infer new vectors for each document of\n",
        "the training corpus, compare the inferred vectors with the training corpus,\n",
        "and then returning the rank of the document based on self-similarity.\n",
        "Basically, we're pretending as if the training corpus is some new unseen data\n",
        "and then seeing how they compare with the trained model. The expectation is\n",
        "that we've likely overfit our model (i.e., all of the ranks will be less than\n",
        "2) and so we should be able to find similar documents very easily.\n",
        "Additionally, we'll keep track of the second ranks for a comparison of less\n",
        "similar documents.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9uRFHFLP3O3"
      },
      "source": [
        "ranks = []\n",
        "second_ranks = []\n",
        "for doc_id in range(len(vocabulary)):\n",
        "    inferred_vector = model.infer_vector(vocabulary[doc_id].words) # uses the model to create the vector\n",
        "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv)) # similaridade  \n",
        "    rank = [docid for docid, sim in sims].index(doc_id) \n",
        "    ranks.append(rank)\n",
        "\n",
        "    second_ranks.append(sims[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6RRAMLdP3O3"
      },
      "source": [
        "Let's count how each document ranks with respect to the training corpus\n",
        "\n",
        "NB. Results vary between runs due to random seeding and very small corpus\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpP7zuIqP3O3",
        "outputId": "d0a55a2f-42f3-414f-91de-6c6232b67f2c"
      },
      "source": [
        "import collections\n",
        "\n",
        "counter = collections.Counter(ranks)\n",
        "print(counter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({0: 498, 1: 2})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOlT3e9YP3O3"
      },
      "source": [
        "Basically, greater than 95% of the inferred documents are found to be most\n",
        "similar to itself and about 5% of the time it is mistakenly most similar to\n",
        "another document. Checking the inferred-vector against a\n",
        "training-vector is a sort of 'sanity check' as to whether the model is\n",
        "behaving in a usefully consistent manner, though not a real 'accuracy' value.\n",
        "\n",
        "This is great and not entirely surprising. We can take a look at an example:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "co4NDhLWP3O4",
        "outputId": "a9317f8c-7817-4995-8e77-cc1f9cd605e5"
      },
      "source": [
        "print('Document ({}): Â«{}Â»\\n'.format(doc_id, ' '.join(vocabulary[doc_id].words)))\n",
        "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
        "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
        "    print(u'%s %s: Â«%sÂ»\\n' % (label, sims[index], ' '.join(vocabulary[sims[index][0]].words)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document (499): Â«rotary positive displacement pump of the internally meshing screw type having stator rotor and an inlet or outlet chamber at one end of the rotor has drive comprising connecting member rigidly secured to the rotor and extending through the chamber which connecting member beyond the end of the chamber remote from the rotor is joined by connecting rod with two universal joints to drive shaft connecting member in the chamber being supported by bearing flexibly carried on resilient support member which is sealed to the outer race of the bearing and also to the chamber wallÂ»\n",
            "\n",
            "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n",
            "\n",
            "MOST (499, 0.9197404980659485): Â«rotary positive displacement pump of the internally meshing screw type having stator rotor and an inlet or outlet chamber at one end of the rotor has drive comprising connecting member rigidly secured to the rotor and extending through the chamber which connecting member beyond the end of the chamber remote from the rotor is joined by connecting rod with two universal joints to drive shaft connecting member in the chamber being supported by bearing flexibly carried on resilient support member which is sealed to the outer race of the bearing and also to the chamber wallÂ»\n",
            "\n",
            "SECOND-MOST (496, 0.7030658721923828): Â«gas compressor or blower comprising cylinder having air intake and exhaust ports piston which is rotatably and reciprocably movable in the cylinder valve means for admitting air into and exhausting air from at least one chamber lying to one side of the piston piston shaft to which the piston is secured the piston shaft passing through seals at the axial ends of the cylinder means for applying rotary drive to the piston rod and cam arrangement for causing reciprocation of the piston rod as this rotates the arrangement of ports and valve means being such that as the piston rotates and reciprocates air is induced into the chamber is compressed in the chamber and then exhausted from the chamberÂ»\n",
            "\n",
            "MEDIAN (260, 0.18387368321418762): Â«pneumatic tire and wheel assembly comprises an enclosing means for lubricating material which releases the lubricating material into the tire when the tire becomes deflated mass counterbalances the mass of the enclosing means and the lubricating material so that when the lubricating material is released the assembly becomes out of balance of which the following is specificationÂ»\n",
            "\n",
            "LEAST (181, -0.2756841778755188): Â«display apparatus for facilitating adjustment of printing press displays several quantities related to the inking of the printing press in display that is arranged to show functional relationship between the quantities the settings of plurality of ink adjustment devices are shown in linear array of setting displays that are arranged in the same order as the adjustment devices are arranged across the width of the press each setting display comprises column of equally spaced dots with an interpolation dot at the end of the column in vertical alignment with and in appropriate lateral relationship with that display of settings further display is provided showing deviations of the printed ink density from set points representing proper ink density each deviation display corresponds to lateral portion of the press whose printed ink density is controlled by the settings of those of the adjustment devices with whose setting display it is alignedÂ»\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jplVevhMP3O4"
      },
      "source": [
        "Notice above that the most similar document (usually the same text) is has a\n",
        "similarity score approaching 1.0. However, the similarity score for the\n",
        "second-ranked documents should be significantly lower (assuming the documents\n",
        "are in fact different) and the reasoning becomes obvious when we examine the\n",
        "text itself.\n",
        "\n",
        "We can run the next cell repeatedly to see a sampling other target-document\n",
        "comparisons.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNZwlfHJnKOD",
        "outputId": "64e72fe9-a135-4da4-886a-becb5d0fe77e"
      },
      "source": [
        "print(train_data.iloc[499]['Titulo'],' | ',train_data.iloc[499]['Resumo'])\n",
        "\n",
        "print(train_data.iloc[496]['Titulo'],' | ',train_data.iloc[496]['Resumo'])\n",
        "\n",
        "print(train_data.iloc[181]['Titulo'],' | ',train_data.iloc[181]['Resumo'])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Rotary displacement pumps   |   A rotary positive displacement pump of the internally-meshing screw type having a stator, a rotor and an inlet or outlet chamber at one end of the rotor has a drive comprising a connecting member rigidly secured to the rotor and extending through the chamber, which connecting member, beyond the end of the chamber remote from the rotor, is joined by a connecting rod with two universal joints to a drive shaft, connecting member in the chamber being supported by a bearing flexibly carried on a resilient support member which is sealed to the outer race of the bearing and also to the chamber wall.\n",
            " Apparatus for use as a gas compressor or gas blower   |   A gas compressor or blower comprising a cylinder having air intake and exhaust ports; a piston which is rotatably and reciprocably movable in the cylinder; valve means for admitting air into and exhausting air from at least one chamber lying to one side of the piston; a piston shaft to which the piston is secured, the piston shaft passing through seals at the axial ends of the cylinder; means for applying a rotary drive to the piston rod; and a cam arrangement for causing reciprocation of the piston rod as this rotates; the arrangement of ports and valve means being such that as the piston rotates and reciprocates air is induced into the chamber, is compressed in the chamber and then exhausted from the chamber.\n",
            " Dual purpose display for printing presses   |   A display apparatus for facilitating adjustment of a printing press displays several quantities related to the inking of the printing press in a display that is arranged to show a functional relationship between the quantities. The settings of a plurality of ink adjustment devices are shown in a linear array of setting displays that are arranged in the same order as the adjustment devices are arranged across the width of the press. Each setting display comprises a column of equally-spaced dots with an interpolation dot at the end of the column. In vertical alignment with and in appropriate lateral relationship with that display of settings, a further display is provided showing deviations of the printed ink density from set points representing proper ink density. Each deviation display corresponds to a lateral portion of the press whose printed ink density is controlled by the settings of those of the adjustment devices with whose setting display it is aligned.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOdicEGFP3O4",
        "outputId": "f1a4baff-00ff-4f54-ffd6-13721a11086d"
      },
      "source": [
        "# Pick a random document from the corpus and infer a vector from the model\n",
        "import random\n",
        "doc_id = random.randint(0, len(train_corpus) - 1)\n",
        "\n",
        "# Compare and print the second-most-similar document\n",
        "print('Train Document ({}): Â«{}Â»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
        "sim_id = second_ranks[doc_id]\n",
        "print('Similar Document {}: Â«{}Â»\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Document (79): Â«dozens of people were injured some seriously and others were trapped after roof collapsed at south african shopping centre burying some children on skating rink witnesses and police said initial reports spoke of up to people trapped at the kolonnade shopping centre in the north of the capital but pretoria emergency spokesman johan pieterse later said police had rescued four people from the rubble and could not immediately locate any more police and police dogs are still inside but can find anyone else just now he added by pm local time am aedt two hours after the collapse injured people mostly adults had been taken to hospitals around the city by ambulance or helicopter mr pieterse said most of those injured were those standing at the glass wall watching people ice skate he added some of the injured included children skating on the ice rink who were partially buried in rubble when the roof gave way witnesses said some square metres of roofing were believed to have collapsed new floor had recently been added to the shopping mall above the rink and the roof had leaked but according to us everything was fine ice rink manager brian bellis told the afp news agency there was lot of dust and smoke and then people were just running and screaming running away from where it happened the south african broadcasting corporation radio quoted one witness as saying several people including young children were brought out on stretchers while others were helicoptered to hospital about police and soldiers cordoned off the area one bystander marius duplessis said he was searching for his year old son who had been working at the centre his car is there but he is nowhere to be found he is not on the casualty list so just praying that he not inside he saidÂ»\n",
            "\n",
            "Similar Document (109, 0.7665959000587463): Â«fire has damaged part of st john the divine cathedral in new york one of the world largest cathedrals new york firefighters battled the blaze for four hours before bringing it under control fire officials say there were no reported injuries but the cathedral gift shop had been badly damaged and the sanctuary suffered some smoke and water damage the fire started at around am on tuesday local time in the church gift shop but around firemen were able to stem the flames spread preventing major damage to the sanctuary itself thick black smoke and bright orange flames had billowed from the immense structure in north western manhattan near the columbia university campus the cause of the fire is as yet unknown firefighter robert savarese who was among the first to enter the church said the main problem was visibility we knew which direction the fire was coming from and we just went toward it he said mr savarese was among group of firemen inside the building when third division deputy chief edward dennehy who was standing outside noticed flames pouring out of the building roof the guys in side thought it was small fire because they could not see through the thick smoke mr dennehy said the ceiling could have collapsed the firefighters battling the blaze inside were pulled out and the fire was subsequently attacked with thick fire hoses from atop tall craneÂ»\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apdYSVonP3O4"
      },
      "source": [
        "Testing the Model\n",
        "-----------------\n",
        "\n",
        "Using the same approach above, we'll infer the vector for a randomly chosen\n",
        "test document, and compare the document to our model by eye.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fm8L4_fPP3O4",
        "outputId": "37a5473b-bc50-47ed-e909-2edb644ec24a"
      },
      "source": [
        "# Pick a random document from the test corpus and infer a vector from the model\n",
        "doc_id = random.randint(0, len(test_corpus) - 1)\n",
        "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
        "sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
        "\n",
        "# Compare and print the most/median/least similar documents from the train corpus\n",
        "print('Test Document ({}): Â«{}Â»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
        "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
        "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
        "    print(u'%s %s: Â«%sÂ»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Document (8): Â«hunan province remained on high alert last night as thunderstorms threatened to exacerbate the flood crisis now entering its fifth day and with already dead and hundreds of thousands evacuated on the flood frontline at dongting lake the water level peaked at just under on saturday night then eased about cm during the day under hot sun with temperatures reaching but with the lake still brimming at dangerously high levels and spilling over the top of its banks in some places locals were fearful that thunderstorm and high winds forecast to hit the region last night would damage the dikes about km of dikes around the lake are all that stand between million people in the surrounding farmland and disasterÂ»\n",
            "\n",
            "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n",
            "\n",
            "MOST (10, 0.6962526440620422): Â«work is continuing this morning to restore power supplies to tens of thousands of homes that were blacked out during wild storms that struck south east queensland last night gale force winds uprooted trees and brought down power lines damaging homes and cars energex and ergon energy have had every available person working through the night to restore power at locations in and around brisbane west to toowoomba and north to the sunshine coast at boonah south west of brisbane protective tarpaulins were ripped from homes still undergoing repairs following severe storms just before christmas at nambour four people were rescued after high voltage power lines fell across their car trapping them inside and at landsborough fierce winds sent large tree crashing into house but no one was injuredÂ»\n",
            "\n",
            "MEDIAN (31, 0.2502707540988922): Â«there is one in chance of dramatic rise in world sea levels over the next century due to global warming according to new risk assessment the survey by the british antarctic survey bas and norwegian environmental safety organisation det norske veritas says there is per cent chance of the giant west antarctic ice sheet disintegrating due to climate change scientist david vaughan says if that happens sea levels would rise by one metre in the next years you have to balance the likelihood against the severity of the impacts and in this case even per cent chance of this happening is really damn serious he said scientists have already predicted rise in sea levels of centimetres over the next century due to combination of climate change and increased extraction of ground water mr vaughan says that estimate did not factor in melting antarctic ice so we might be looking at something like metres in the next century mr vaughan said mr vaughan says the possible break up of the west antarctic ice sheet which accounts for per cent of ice on the frozen continent is not related to the impact of human industrial activity on the climate he says it is part of far older process however he says major world polluters cannot walk away from the problem the potential impacts of major change in the west antarctic ice sheet are severe sea level rise will be fantastically expensive for developed nations with coastal cities and dire for poor populations in low lying coastal areas mr vaughan saidÂ»\n",
            "\n",
            "LEAST (265, -0.04385971277952194): Â«the federal government is under fire from unions over new departmental report which recommends australia outsource information technology it to india the document says india has low cost skilled workforce the minister for foreign affairs and trade alexander downer has given his support to the document from his department entitled india new economy old economy the report says sectors like it finance and offer attractive direct investment opportunities it also says australian firms could become more competitive by outsourcing to the indian it sector the community and public sector union wendy caird says the government seems to be encouraging local companies to export jobs to india think that quite alarming obviously labour is great deal cheaper in india and that assisted by the indian government removing labour laws and bankruptcy laws ms caird said the union says while the initiative may create jobs in india it will not help australia rising unemploymentÂ»\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYcadchUP3O4"
      },
      "source": [
        "Conclusion\n",
        "----------\n",
        "\n",
        "Let's review what we've seen in this tutorial:\n",
        "\n",
        "0. Review the relevant models: bag-of-words, Word2Vec, Doc2Vec\n",
        "1. Load and preprocess the training and test corpora (see `core_concepts_corpus`)\n",
        "2. Train a Doc2Vec `core_concepts_model` model using the training corpus\n",
        "3. Demonstrate how the trained model can be used to infer a `core_concepts_vector`\n",
        "4. Assess the model\n",
        "5. Test the model on the test corpus\n",
        "\n",
        "That's it! Doc2Vec is a great way to explore relationships between documents.\n",
        "\n",
        "Additional Resources\n",
        "--------------------\n",
        "\n",
        "If you'd like to know more about the subject matter of this tutorial, check out the links below.\n",
        "\n",
        "* `Word2Vec Paper <https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf>`_\n",
        "* `Doc2Vec Paper <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`_\n",
        "* `Dr. Michael D. Lee's Website <http://faculty.sites.uci.edu/mdlee>`_\n",
        "* `Lee Corpus <http://faculty.sites.uci.edu/mdlee/similarity-data/>`__\n",
        "* `IMDB Doc2Vec Tutorial <doc2vec-IMDB.ipynb>`_\n",
        "\n",
        "\n"
      ]
    }
  ]
}